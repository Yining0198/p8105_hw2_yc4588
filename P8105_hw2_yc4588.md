p8105_hw2_yc4588
================
Yining Cao
2024-09-30

# Problem 1

## Import and clean the dataset

``` r
library(readr)
library(dplyr)
```

    ## 
    ## Attaching package: 'dplyr'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     filter, lag

    ## The following objects are masked from 'package:base':
    ## 
    ##     intersect, setdiff, setequal, union

``` r
ent_exit = read_csv("NYC_Transit_Subway_Entrance_And_Exit_Data.csv",na = c("NA", "", "."))
```

    ## Rows: 1868 Columns: 32

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
colnames(ent_exit)
```

    ##  [1] "Division"           "Line"               "Station Name"      
    ##  [4] "Station Latitude"   "Station Longitude"  "Route1"            
    ##  [7] "Route2"             "Route3"             "Route4"            
    ## [10] "Route5"             "Route6"             "Route7"            
    ## [13] "Route8"             "Route9"             "Route10"           
    ## [16] "Route11"            "Entrance Type"      "Entry"             
    ## [19] "Exit Only"          "Vending"            "Staffing"          
    ## [22] "Staff Hours"        "ADA"                "ADA Notes"         
    ## [25] "Free Crossover"     "North South Street" "East West Street"  
    ## [28] "Corner"             "Entrance Latitude"  "Entrance Longitude"
    ## [31] "Station Location"   "Entrance Location"

- The variables in the dataset include: *Division*(BMT, IND, etc),
  *Line*(“4 avenue”, “42nd St Shuttle”, “6 avenue”, etc.), *Station
  Name*(“25th St”, “36th St”, “45th St”, etc) and so on.

``` r
library(tidyr)
ent_exit_df <- ent_exit |>
  janitor::clean_names()|>
  mutate(entry = case_match(
    entry,
    "YES" ~ TRUE,
    "NO" ~ FALSE
  )) |>
  select(line,
         station_name, 
         station_latitude, 
         station_longitude, 
         starts_with("route"),
         entry, 
         vending, 
         entrance_type, 
         ada)
```

- The dataset contains information about subway entrances and exits in
  New York City, with variables including *line*, *station_name*,
  *station_latitude*, *station_longitude*, *route1-route11*, *entry*,
  *vending*, *entrance_type*, and *ada*.
- cleaning steps:
  1.  Clean column names using the `janitor::clean_names()` function.
  2.  Convert the `entry` column to a logical variable using the
      `case_match()` function.
  3.  Select the columns of interest with `select()` function, including
      `line`, `station_name`, `station_latitude`, `station_longitude`,
      `route1` to `route11`, `entry`, `vending`, `entrance_type`, and
      `ada`.
- This dataset contains 1868 rows and 19 columns after cleaning.
- These data are not tidy enough because the `route1` to `route11`
  columns are not in a long format.

## How many distinct satation are here?

``` r
distinct_stations <- ent_exit_df |> 
  distinct(line, station_name) |> 
  nrow()
```

- There are 465 distinct stations in the dataset.

## How many stations are ADA compliant?

``` r
ada_stations <- ent_exit_df |>
  filter(ada == "TRUE") |>
  distinct(station_name, line) |>
  nrow()
```

- There are 84 ADA-compliant stations in the dataset.

## What proportion of station entrances / exits without vending allow entrance?

``` r
no_vending_entry <- ent_exit_df |>
  filter(vending == "NO") |>
  summarise(prop_entry = mean(entry, na.rm = TRUE))
```

- The proportion of station entrances/exits without vending that allow
  entrance is 0.3770492.

## Reformat the data so that route number and route name are distinct

``` r
ent_exit_df <- ent_exit_df |>
  mutate(across(starts_with("route"), as.character)) |> 
  pivot_longer(cols = starts_with("route"), 
               names_to = "route_number", 
               values_to = "route_name")
```

## How many distinct stations serve the A train?

``` r
A_train_stations <- ent_exit_df |>
  filter(route_name == "A") |>
  distinct(station_name, line) |>
  nrow()
```

- There are 60 distinct stations that serve the A train.

## Of the stations that serve the A train, how many are ADA compliant?

``` r
A_train_ada_stations <- ent_exit_df |>
  filter(route_name == "A") |>
  filter(ada == "TRUE") |>
  distinct(station_name, line) |>
  nrow()
```

- There are 17 ADA-compliant stations that serve the A train.

# Problem 2

## Import and clean the *Mr. Trash Wheel * dataset

``` r
library(readxl)
Mr_trash_wheel =  readxl::read_excel("202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N586",na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |> 
  mutate(sports_balls = as.integer(round(sports_balls)))
```

## Import and clean the *Professor Trash Wheel* dataset and the *Gwynnda Trash Wheel* dataset

``` r
Professor_trash_wheel =  readxl::read_excel("202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", range = "A2:M108",na = c("NA", "", ".")) |>
  janitor::clean_names()|>
  filter(!is.na(dumpster))

Gwynnda_trash_wheel =  readxl::read_excel("202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L157",na = c("NA", "", ".")) |>
  janitor::clean_names()|>
  filter(!is.na(dumpster))
```

## Combine the three datasets

``` r
# switch the year column of Mr. Trash Whell dataset to integer
Mr_trash_wheel <- Mr_trash_wheel |> 
  mutate(year = as.integer(year))

trash_wheel_df <- bind_rows(
  Mr_trash_wheel |>
    mutate(trash_wheel = "Mr. Trash Wheel"),
  Professor_trash_wheel |>
    mutate(trash_wheel = "Professor Trash Wheel"),
  Gwynnda_trash_wheel |>
    mutate(trash_wheel = "Gwynnda Trash Wheel")
)
```

- The combined dataset integrates information from three distinct trash
  wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash
  Wheel.
- The resulting dataset contains 845 observations and 15 variables.
- The variables in the dataset include: *dumpster*(Mr:584;
  Professor:107; Gwynnda:156), *month*(January - December),
  *year*(Mr:2014-2023; Professor:2017-2023; Gwynnda:2021-2023) and so
  on.
- The total weight of trash collected by Professor Trash Wheel is 216.26
  tons.
- In June 2022, Gwynnda collected a total of 40.53 tons of trash.

# Problem 3

## Import and clean three datesets of individual bakers, their bakes, and their performance

``` r
bakers = read_csv("gbb_datasets/bakers.csv",na = c("NA", "", ".")) |>
  janitor::clean_names()|>
  mutate(
    baker_name = sub(" .*", "", baker_name)
  ) |>
  rename(baker = baker_name)|>
  mutate(
    baker = replace(baker, baker == "Jo", "Joanne")
  )
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes = read_csv("gbb_datasets/bakes.csv", col_names = TRUE, na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  mutate(
    baker = replace(baker, baker == '"Jo"', "Joanne")
  )
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results = read_csv("gbb_datasets/results.csv",skip = 2, na = c("N/A", "NA", "UNKNOWN","", "Unknown")) |>
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## Merge the three datasets

``` r
 merged_data = 
  left_join(results, bakes, 
            by = join_by(series, episode, baker)) |>
  left_join(bakers,
            by = join_by(series, baker)) |>
  select(baker, everything())
```

## Checking for completeness and correctness across datasets

``` r
anti_join(results, merged_data,
          by = join_by(series, episode, baker, technical, result))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>, technical <dbl>,
    ## #   result <chr>

``` r
anti_join(bakes, merged_data,
          by = join_by(series, episode, baker, signature_bake,show_stopper))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
anti_join(bakers, merged_data, 
          by = join_by(baker,series, baker_age,baker_occupation, hometown))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: baker <chr>, series <dbl>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>

- These datasets are complete and correct because the `anti_join()`
  function returns an empty data frame for all three datasets.

## Export the merged dataset

``` r
write.csv(merged_data, "gbb_datasets/merged_data.csv", row.names = FALSE)
```

- Cleaning steps:
  1.  Clean column names using the `janitor::clean_names()` function.
  2.  Remove the last name from the `baker_name` column using the
      `sub()` function.
  3.  Rename the `baker_name` column to `baker`.
  4.  Replace the name “Jo” with “Joanne” in the `baker`columns of all
      three datasets.
  5.  Skip the first two rows in the `results` dataset using the `skip`
      argument.
  6.  After merging the datasets, check for completeness and correctness
      using the `anti_join()` function.
- The merged dataset contains 1136 observations and 10 variables.
- The variables in the dataset include: *baker*, *series*, *episode*,
  *technical*, *result*, *signature_bake*, *show_stopper*, *baker_age*,
  *baker_occupation*, and *hometown*.

## Creating a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10

``` r
star_baker_table = merged_data |>
  filter(result %in% c("STAR BAKER", "WINNER")) |>
  filter(series %in% c(5:10)) |> 
  arrange(series, episode) |>
  select(episode, baker, series) |>
  pivot_wider(
    names_from = "series",
    values_from = "baker"  ) |>
  rename("series 5" = "5",
         "series 6" = "6",
         "series 7" = "7",
         "series 8" = "8",
         "series 9" = "9",
         "series 10" = "10") |>
  knitr::kable()

star_baker_table
```

| episode | series 5 | series 6 | series 7  | series 8 | series 9 | series 10 |
|--------:|:---------|:---------|:----------|:---------|:---------|:----------|
|       1 | Nancy    | Marie    | Jane      | Steven   | Manon    | Michelle  |
|       2 | Richard  | Ian      | Candice   | Steven   | Rahul    | Alice     |
|       3 | Luis     | Ian      | Tom       | Julia    | Rahul    | Michael   |
|       4 | Richard  | Ian      | Benjamina | Kate     | Dan      | Steph     |
|       5 | Kate     | Nadiya   | Candice   | Sophie   | Kim-Joy  | Steph     |
|       6 | Chetna   | Mat      | Tom       | Liam     | Briony   | Steph     |
|       7 | Richard  | Tamal    | Andrew    | Steven   | Kim-Joy  | Henry     |
|       8 | Richard  | Nadiya   | Candice   | Stacey   | Ruby     | Steph     |
|       9 | Richard  | Nadiya   | Andrew    | Sophie   | Ruby     | Alice     |
|      10 | Nancy    | Nadiya   | Candice   | Sophie   | Rahul    | David     |

- Richard won the star baker or winner title 5 times in series 5.
- Nadiya won the star baker or winner title 4 times in series 6.
- Candice won the star baker or winner title 4 times in series 7.
- Steven and Sophie won the star baker or winner title 3 times in series
  8, respectively.
- Rahul won the star baker or winner title 3 times in series 9.
- Steph won the star baker or winner title 4 times in series 10.
- The winners, such as Richard, Candice, and Steven, are overall
  predictable, while the winners, such as Nadiya, Sophie, and Steph, are
  relatively unexpected because they won fewer star baker titles or
  winner titles in the early episodes of their respective series, which
  may surprised the audience.

## Import and clean viewers dataset

``` r
viewers = read_csv("gbb_datasets/viewers.csv", na = c("NA", "", ".")) |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## What was the average viewership in Season 1? In Season 5?

``` r
head(viewers,10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

- The average viewership in Season 1 was 2.77 million.
- The average viewership in Season 5 was 10.0393 million.
